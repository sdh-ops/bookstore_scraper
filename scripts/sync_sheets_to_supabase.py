import gspread
from google.oauth2.service_account import Credentials
import pandas as pd
import os
import json
import argparse
import re
from supabase import create_client, Client

def clean_int(val):
    if pd.isna(val) or val == '' or val is None:
        return 0
    if isinstance(val, (int, float)):
        return int(val)
    return int(re.sub(r'[^\d-]', '', str(val)) or 0)

# Configuration
GOOGLE_CREDENTIALS_JSON = os.environ.get('GOOGLE_CREDENTIALS')
SUPABASE_URL = os.environ.get('SUPABASE_URL')
SUPABASE_KEY = os.environ.get('SUPABASE_SERVICE_ROLE_KEY')

# Sheet IDs
SALES_SHEET_ID = '1bH7D7zO56xzp555BGiVCB1Mo5cRLxqN7GkC_Tudqp8s'
K_PUB_SHEET_ID = '1EfxiIat1bEUXOfdyPS184yY7ublnZVoZ7P81xMIouaE'
COSTS_SHEET_ID = '1okyT7AfjOAmYwIxA-ffQb7NGnlYIRhjrF9Kc3L77Tc8'

def clean_isbn(val):
    if pd.isna(val) or val is None:
        return None
    # Handle float-like strings from Excel/Sheets (e.g., "978...0")
    s = str(val).strip()
    if '.' in s:
        s = s.split('.')[0]
    return re.sub(r'[^0-9X]', '', s)

def get_gspread_client():
    import re # Ensure re is available if needed, though it's imported at top

    scopes = [
        "https://www.googleapis.com/auth/spreadsheets",
        "https://www.googleapis.com/auth/drive"
    ]
    creds_dict = json.loads(GOOGLE_CREDENTIALS_JSON)
    creds = Credentials.from_service_account_info(creds_dict, scopes=scopes)
    return gspread.authorize(creds)

def sync_store_sales():
    """Syncs 'í†µí•©í…Œì´ë¸”' from the Bookstore Scraper sheet to Supabase."""
    print("Starting Store Sales Sync...")
    gc = get_gspread_client()
    sh = gc.open_by_key(SALES_SHEET_ID)
    worksheet = sh.worksheet("í†µí•©í…Œì´ë¸”")
    
    data = worksheet.get_all_records()
    df = pd.DataFrame(data)
    
    if df.empty:
        print("No data found in store sales sheet.")
        return

    # Clean columns
    df['ISBN'] = df['ISBN'].apply(clean_isbn)
    df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ']).dt.strftime('%Y-%m-%d')
    
    bookstores = ['êµë³´ê³„', 'YES24', 'ì•Œë¼ë”˜', 'ì˜í’']
    melted = df.melt(id_vars=['ë‚ ì§œ', 'ISBN', 'ì •ê°€'], 
                     value_vars=[b for b in bookstores if b in df.columns], 
                     var_name='bookstore', 
                     value_name='quantity')
    
    # ğŸ” Deduplicate/Aggregate before upsert
    agg_df = melted.groupby(['ë‚ ì§œ', 'ISBN', 'bookstore']).agg({
        'quantity': 'sum',
        'ì •ê°€': 'first' # Assuming price is consistent
    }).reset_index()
    
    records = []
    for _, row in agg_df.iterrows():
        records.append({
            "isbn": row['ISBN'],
            "sale_date": row['ë‚ ì§œ'],
            "bookstore": row['bookstore'].replace('ê³„', ''),
            "quantity": clean_int(row['quantity']),
            "price": clean_int(row['ì •ê°€'])
        })
    
    upsert_to_supabase(records, "daily_sales")

def sync_k_pub_sales():
    """Syncs data from K-Publishing (ë¬¸í™”ìœ í†µ) sheet to Supabase."""
    print("Starting K-Publishing Sales Sync...")
    gc = get_gspread_client()
    sh = gc.open_by_key(K_PUB_SHEET_ID)
    
    # 1. Load Dimensions and Fact
    print("Fetching sheets (agg_sales_daily, dim_books, dim_dates)...")
    sales_df = pd.DataFrame(sh.worksheet("agg_sales_daily").get_all_records())
    books_df = pd.DataFrame(sh.worksheet("dim_books").get_all_records())
    dates_df = pd.DataFrame(sh.worksheet("dim_dates").get_all_records())
    
    if sales_df.empty or books_df.empty or dates_df.empty:
        print("Required sheets are empty.")
        return
        
    # 2. Join to get ISBN and Date
    # Mapping: sales.book_id -> books.book_id (to get ISBN)
    # Mapping: sales.date_id -> dates.date_id (to get actual date)
    
    # Clean dim_books: Keep only book_id and ISBN
    books_lookup = books_df[['book_id', 'ISBN']].copy()
    books_lookup['ISBN'] = books_lookup['ISBN'].apply(clean_isbn)
    
    # Clean dim_dates: Keep only date_id and date
    dates_lookup = dates_df[['date_id', 'date']].copy()
    
    # Perform Joins
    merged = sales_df.merge(books_lookup, on='book_id', how='left')
    merged = merged.merge(dates_lookup, on='date_id', how='left')
    
    print(f"Merged {len(merged)} rows. Filtering and mapping...")
    
    # 3. Filter and Map to DB schema
    merged = merged[pd.notnull(merged['ISBN']) & pd.notnull(merged['date'])]
    
    # ğŸ” Deduplicate/Aggregate: Group by (ISBN, date, bookstore) to avoid "ON CONFLICT" errors in Postgres
    # ğŸ” Aggregation to prevent ON CONFLICT error
    merged['date'] = pd.to_datetime(merged['date']).dt.strftime('%Y-%m-%d')
    agg_df = merged.groupby(['ISBN', 'date']).agg({
        'total_quantity': 'sum',
        'total_amount': 'sum'
    }).reset_index()
    
    # 3. Filter and Map to DB schema
    records = []
    for _, row in agg_df.iterrows():
        if pd.isna(row['ISBN']) or pd.isna(row['date']):
            continue
            
        records.append({
            "isbn": row['ISBN'],
            "sale_date": row['date'],
            "bookstore": "ë¬¸í™”ìœ í†µDB",
            "quantity": int(row['total_quantity']),
            "price": int(row['total_amount'] / row['total_quantity']) if row['total_quantity'] > 0 else 0
        })
    
    print(f"Prepared {len(records)} unique records for Supabase.")
    upsert_to_supabase(records, "daily_sales")

def sync_inventory():
    """Syncs inventory data from Google Sheets to Supabase."""
    print("Starting Inventory Sync...")
    gc = get_gspread_client()
    sh = gc.open_by_key(K_PUB_SHEET_ID)
    
    print("Fetching sheets (ì¬ê³ í˜„í™©, dim_books)...")
    inv_df = pd.DataFrame(sh.worksheet("ì¬ê³ í˜„í™©").get_all_records())
    books_df = pd.DataFrame(sh.worksheet("dim_books").get_all_records())
    
    sb = create_client(SUPABASE_URL, SUPABASE_KEY)
    gc = get_gspread_client()
    sh = gc.open_by_key(K_PUB_SHEET_ID)
    
    print("Fetching sheets (inventory, dim_books)...")
    inv_df = pd.DataFrame(sh.worksheet("ì¬ê³ í˜„í™©").get_all_records()) # Changed to "inventory" in snippet, but keeping "ì¬ê³ í˜„í™©" as per original
    mapping_df = pd.DataFrame(sh.worksheet("dim_books").get_all_records())
    
    if inv_df.empty or mapping_df.empty:
        print("Inventory or books sheet is empty.")
        return

    # 1. Mapping: book_id -> ISBN
    mapping = dict(zip(mapping_df['book_id'].astype(str), mapping_df['ISBN'].astype(str)))
    
    records = []
    seen_isbns = set()
    for _, row in inv_df.iterrows():
        book_id = str(row.get('book_id'))
        isbn = clean_isbn(mapping.get(book_id))
        if not isbn or isbn in seen_isbns:
            continue
            
        # Ensure book exists in master table
        ensure_book_exists(sb, isbn)
            
        # êµ¬ê¸€ ì‹œíŠ¸ í—¤ë”ê°€ í•œê¸€ì¸ ê²½ìš°ì™€ ì˜ë¬¸ì¸ ê²½ìš° ëª¨ë‘ ëŒ€ì‘
        stock_normal = clean_int(row.get('normal_stock') or row.get('ì •ìƒì¬ê³ '))
        stock_return = clean_int(row.get('return_stock') or row.get('ë°˜í’ˆì¬ê³ '))
        stock_hq = clean_int(row.get('hq_stock') or row.get('ë³¸ì‚¬ì¬ê³ '))
        # stock_logisticsëŠ” ì‹œíŠ¸ì˜ 'ì „ì²´ì¬ê³ ' ë˜ëŠ” 'ì¬ê³ í•©ê³„' ì»¬ëŸ¼ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ í•©ì‚°
        stock_logistics = clean_int(row.get('total_stock') or row.get('ì „ì²´ì¬ê³ '))
        
        if stock_logistics == 0:
            stock_logistics = stock_normal + stock_return + stock_hq
            
        if stock_logistics > 0:
            # ì¬ê³ ê°€ ë°œìƒí•œ ê²½ìš° books í…Œì´ë¸”ì˜ ëª¨ë“  ìƒíƒœ í•„ë“œë¥¼ 'íŒë§¤ì¤‘'ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            try:
                sb.table('books').update({
                    "status": "íŒë§¤ì¤‘",
                    "sales_status": "íŒë§¤ì¤‘",
                    "publication_stage": "ì¶œê°„"
                }).eq("isbn", isbn).execute()
            except Exception as e:
                print(f"Status update error for {isbn}: {e}")

        records.append({
            "isbn": isbn,
            "stock_normal": stock_normal,
            "stock_return": stock_return,
            "stock_hq": stock_hq,
            "stock_logistics": stock_logistics
        })
        seen_isbns.add(isbn)
    
    print(f"Prepared {len(records)} inventory records.")
    upsert_to_supabase(records, "inventory")

def sync_production_costs():
    """Syncs production cost data from Google Sheets ('ì œì‘ë¹„ ì›”ë³„ ì„¸ë¶„í™”ì‘ì—… data') to Supabase."""
    print("Starting Production Costs Sync...")
    gc = get_gspread_client()
    sh = gc.open_by_key(COSTS_SHEET_ID)
    ws = sh.worksheet("data")
    
    data = ws.get_all_records()
    df = pd.DataFrame(data)
    
    if df.empty:
        print("No data found in production costs sheet.")
        return

    # Clean and Map columns
    # The sheet is expected to have columns like 'ISBN', 'ì œì‘ì›”', 'íŒìˆ˜', etc.
    # We will map them to the production_costs table columns.
    
    records = []
    for _, row in df.iterrows():
        isbn = clean_isbn(row.get('ISBN') or row.get('isbn'))
        if not isbn:
            continue
            
        records.append({
            "isbn": isbn,
            "production_month": str(row.get('ì œì‘ì›”') or row.get('ì œì‘ë‹¬') or ''),
            "edition": str(row.get('íŒìˆ˜') or row.get('íŒ') or '1'),
            "book_title": row.get('ë„ì„œëª…') or row.get('ì„œëª…'),
            "print_qty": clean_int(row.get('ë°œì£¼ë¶€ìˆ˜') or row.get('ì¸ì‡„ë¶€ìˆ˜')),
            "receive_qty": clean_int(row.get('ì…ê³ ë¶€ìˆ˜')),
            "list_price": clean_int(row.get('ì •ê°€')),
            "cost_paper": clean_int(row.get('ìš©ì§€ë¹„') or row.get('ì¢…ì´ê°’')),
            "cost_ctp": clean_int(row.get('ì¶œë ¥ë¹„') or row.get('CTP')),
            "cost_body_print": clean_int(row.get('ë³¸ë¬¸ì¸ì‡„ë¹„') or row.get('ë³¸ë¬¸ì¸ì‡„')),
            "cost_cover_print": clean_int(row.get('í‘œì§€ì¸ì‡„ë¹„') or row.get('í‘œì§€ì¸ì‡„')),
            "cost_coating": clean_int(row.get('ì½”íŒ…ë¹„')),
            "cost_finishing": clean_int(row.get('ê°€ê³µë¹„')),
            "cost_binding": clean_int(row.get('ì œë³¸ë¹„')),
            "cost_total": clean_int(row.get('ì œì‘í•©ê³„') or row.get('í•©ê³„')),
            "vendor_paper": row.get('ìš©ì§€ì²˜'),
            "vendor_body_print": row.get('ì¸ì‡„ì²˜'),
            "vendor_binding": row.get('ì œë³¸ì²˜')
        })

    print(f"Prepared {len(records)} production cost records.")
    upsert_to_supabase(records, "production_costs")

def upsert_to_supabase(records, table_name):
    if not records:
        print(f"No records to upsert for {table_name}.")
        return
    
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
    
    on_conflict = "isbn, sale_date, bookstore" if table_name == "daily_sales" else "isbn"
    
    chunk_size = 500
    for i in range(0, len(records), chunk_size):
        chunk = records[i:i + chunk_size]
        print(f"Upserting {table_name} chunk {i//chunk_size + 1} ({len(chunk)} records)...")
        try:
            supabase.table(table_name).upsert(
                chunk, 
                on_conflict=on_conflict
            ).execute()
        except Exception as e:
            print(f"Error during upsert to {table_name}: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Sync Google Sheets to Supabase.')
    parser.add_argument('--source', choices=['bookstore', 'kpub', 'costs', 'all'], required=True, 
                        help='Source of the data (bookstore, kpub, costs, or all)')
    
    args = parser.parse_args()

    if not GOOGLE_CREDENTIALS_JSON or not SUPABASE_URL or not SUPABASE_KEY:
        print("Missing required environment variables (GOOGLE_CREDENTIALS, SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY).")
        exit(1)
        
    if args.source == 'bookstore' or args.source == 'all':
        sync_store_sales()
    if args.source == 'kpub' or args.source == 'all':
        sync_k_pub_sales()
        sync_inventory()
    if args.source == 'costs' or args.source == 'all':
        sync_production_costs()

        
    print(f"Sync process for {args.source} completed.")
