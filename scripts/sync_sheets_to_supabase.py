import gspread
from google.oauth2.service_account import Credentials
import pandas as pd
import os
import json
import argparse
import re
from supabase import create_client, Client

# Configuration
GOOGLE_CREDENTIALS_JSON = os.environ.get('GOOGLE_CREDENTIALS')
SUPABASE_URL = os.environ.get('SUPABASE_URL')
SUPABASE_KEY = os.environ.get('SUPABASE_SERVICE_ROLE_KEY')

# Sheet IDs
SALES_SHEET_ID = '1bH7D7zO56xzp555BGiVCB1Mo5cRLxqN7GkC_Tudqp8s'
K_PUB_SHEET_ID = '1EfxiIat1bEUXOfdyPS184yY7ublnZVoZ7P81xMIouaE'

def clean_isbn(val):
    if pd.isna(val) or val is None:
        return None
    # Handle float-like strings from Excel/Sheets (e.g., "978...0")
    s = str(val).strip()
    if '.' in s:
        s = s.split('.')[0]
    return re.sub(r'[^0-9X]', '', s)

def get_gspread_client():
    import re # Ensure re is available if needed, though it's imported at top

    scopes = [
        "https://www.googleapis.com/auth/spreadsheets",
        "https://www.googleapis.com/auth/drive"
    ]
    creds_dict = json.loads(GOOGLE_CREDENTIALS_JSON)
    creds = Credentials.from_service_account_info(creds_dict, scopes=scopes)
    return gspread.authorize(creds)

def sync_store_sales():
    """Syncs 'í†µí•©í…Œì´ë¸”' from the Bookstore Scraper sheet to Supabase."""
    print("Starting Store Sales Sync...")
    gc = get_gspread_client()
    sh = gc.open_by_key(SALES_SHEET_ID)
    worksheet = sh.worksheet("í†µí•©í…Œì´ë¸”")
    
    data = worksheet.get_all_records()
    df = pd.DataFrame(data)
    
    if df.empty:
        print("No data found in store sales sheet.")
        return

    # Clean columns
    df['ISBN'] = df['ISBN'].apply(clean_isbn)
    df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ']).dt.strftime('%Y-%m-%d')
    
    bookstores = ['êµë³´ê³„', 'YES24', 'ì•Œë¼ë”˜', 'ì˜í’']
    melted = df.melt(id_vars=['ë‚ ì§œ', 'ISBN', 'ì •ê°€'], 
                     value_vars=[b for b in bookstores if b in df.columns], 
                     var_name='bookstore', 
                     value_name='quantity')
    
    # ðŸ” Deduplicate/Aggregate before upsert
    agg_df = melted.groupby(['ë‚ ì§œ', 'ISBN', 'bookstore']).agg({
        'quantity': 'sum',
        'ì •ê°€': 'first' # Assuming price is consistent
    }).reset_index()
    
    records = []
    for _, row in agg_df.iterrows():
        records.append({
            "isbn": row['ISBN'],
            "sale_date": row['ë‚ ì§œ'],
            "bookstore": row['bookstore'].replace('ê³„', ''),
            "quantity": int(row['quantity']),
            "price": int(row['ì •ê°€']) if pd.notnull(row['ì •ê°€']) else 0
        })
    
    upsert_to_supabase(records, "daily_sales")

def sync_k_pub_sales():
    """Syncs data from K-Publishing (ë¬¸í™”ìœ í†µ) sheet to Supabase."""
    print("Starting K-Publishing Sales Sync...")
    gc = get_gspread_client()
    sh = gc.open_by_key(K_PUB_SHEET_ID)
    
    # 1. Load Dimensions and Fact
    print("Fetching sheets (agg_sales_daily, dim_books, dim_dates)...")
    sales_df = pd.DataFrame(sh.worksheet("agg_sales_daily").get_all_records())
    books_df = pd.DataFrame(sh.worksheet("dim_books").get_all_records())
    dates_df = pd.DataFrame(sh.worksheet("dim_dates").get_all_records())
    
    if sales_df.empty or books_df.empty or dates_df.empty:
        print("Required sheets are empty.")
        return
        
    # 2. Join to get ISBN and Date
    # Mapping: sales.book_id -> books.book_id (to get ISBN)
    # Mapping: sales.date_id -> dates.date_id (to get actual date)
    
    # Clean dim_books: Keep only book_id and ISBN
    books_lookup = books_df[['book_id', 'ISBN']].copy()
    books_lookup['ISBN'] = books_lookup['ISBN'].apply(clean_isbn)
    
    # Clean dim_dates: Keep only date_id and date
    dates_lookup = dates_df[['date_id', 'date']].copy()
    
    # Perform Joins
    merged = sales_df.merge(books_lookup, on='book_id', how='left')
    merged = merged.merge(dates_lookup, on='date_id', how='left')
    
    print(f"Merged {len(merged)} rows. Filtering and mapping...")
    
    # 3. Filter and Map to DB schema
    merged = merged[pd.notnull(merged['ISBN']) & pd.notnull(merged['date'])]
    
    # ðŸ” Deduplicate/Aggregate: Group by (ISBN, date, bookstore) to avoid "ON CONFLICT" errors in Postgres
    # ðŸ” Aggregation to prevent ON CONFLICT error
    merged['date'] = pd.to_datetime(merged['date']).dt.strftime('%Y-%m-%d')
    agg_df = merged.groupby(['ISBN', 'date']).agg({
        'total_quantity': 'sum',
        'total_amount': 'sum'
    }).reset_index()
    
    # 3. Filter and Map to DB schema
    records = []
    for _, row in agg_df.iterrows():
        if pd.isna(row['ISBN']) or pd.isna(row['date']):
            continue
            
        records.append({
            "isbn": row['ISBN'],
            "sale_date": row['date'],
            "bookstore": "ë¬¸í™”ìœ í†µDB",
            "quantity": int(row['total_quantity']),
            "price": int(row['total_amount'] / row['total_quantity']) if row['total_quantity'] > 0 else 0
        })
    
    print(f"Prepared {len(records)} unique records for Supabase.")
    upsert_to_supabase(records, "daily_sales")

def sync_inventory():
    """Syncs inventory data from Google Sheets to Supabase."""
    print("Starting Inventory Sync...")
    gc = get_gspread_client()
    sh = gc.open_by_key(K_PUB_SHEET_ID)
    
    print("Fetching sheets (ìž¬ê³ í˜„í™©, dim_books)...")
    inv_df = pd.DataFrame(sh.worksheet("ìž¬ê³ í˜„í™©").get_all_records())
    books_df = pd.DataFrame(sh.worksheet("dim_books").get_all_records())
    
    if inv_df.empty or books_df.empty:
        print("Inventory or books sheet is empty.")
        return

    # 1. Mapping: book_id -> ISBN
    books_lookup = books_df[['book_id', 'ISBN']].copy()
    books_lookup['ISBN'] = books_lookup['ISBN'].apply(clean_isbn)
    mapping = dict(zip(books_lookup['book_id'], books_lookup['ISBN']))

    # 2. Process Inventory (Sort by date if available, but for latest sync we just need latest entries)
    # If snapshot_date exists, we could sort, but usually the sheet has the latest.
    
    records = []
    seen_isbns = set()
    for _, row in inv_df.iterrows():
        book_id = row.get('book_id')
        isbn = mapping.get(book_id)
        if not isbn or isbn in seen_isbns:
            continue
            
        records.append({
            "isbn": isbn,
            "stock_normal": int(row.get('normal_stock', 0) or 0),
            "stock_return": int(row.get('return_stock', 0) or 0),
            "stock_hq": int(row.get('hq_stock', 0) or 0)
        })
        seen_isbns.add(isbn)

    print(f"Prepared {len(records)} inventory records.")
    upsert_to_supabase(records, "inventory")

def upsert_to_supabase(records, table_name):
    if not records:
        print(f"No records to upsert for {table_name}.")
        return
    
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
    
    on_conflict = "isbn, sale_date, bookstore" if table_name == "daily_sales" else "isbn"
    
    chunk_size = 500
    for i in range(0, len(records), chunk_size):
        chunk = records[i:i + chunk_size]
        print(f"Upserting {table_name} chunk {i//chunk_size + 1} ({len(chunk)} records)...")
        try:
            supabase.table(table_name).upsert(
                chunk, 
                on_conflict=on_conflict
            ).execute()
        except Exception as e:
            print(f"Error during upsert to {table_name}: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Sync Google Sheets to Supabase.')
    parser.add_argument('--source', choices=['bookstore', 'kpub'], required=True, 
                        help='Source of the data (bookstore or kpub)')
    
    args = parser.parse_args()

    if not GOOGLE_CREDENTIALS_JSON or not SUPABASE_URL or not SUPABASE_KEY:
        print("Missing required environment variables (GOOGLE_CREDENTIALS, SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY).")
        exit(1)
        
    if args.source == 'bookstore':
        sync_store_sales()
    elif args.source == 'kpub':
        sync_k_pub_sales()
        sync_inventory()  # K-Pub source now includes inventory

        
    print(f"Sync process for {args.source} completed.")
